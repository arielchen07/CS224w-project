{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import osmium\n",
    "from osmium import osm\n",
    "from typing import List, Tuple\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "\n",
    "R = 6371000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(osmPath: str):\n",
    "\n",
    "    def compute_distance(lat1, lon1, lat2, lon2):\n",
    "        phi1 = math.radians(lat1)\n",
    "        phi2 = math.radians(lat2)\n",
    "        dphi = math.radians(lat2 - lat1)\n",
    "        dlambda = math.radians(lon2 - lon1)\n",
    "        a = (math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2)\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        return R * c\n",
    "\n",
    "    class MapCreationHandler(osmium.SimpleHandler):\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.nodes = []\n",
    "            self.edges = [[], []]\n",
    "            self.edge_dist = []\n",
    "            self.node_id_to_idx = {}\n",
    "            self.idx_to_node_id = {}\n",
    "            self.id_counter = 0\n",
    "\n",
    "        def node(self, n: osmium.osm.Node) -> None:\n",
    "            self.nodes.append([n.location.lat, n.location.lon])\n",
    "            self.node_id_to_idx[n.id] = self.id_counter\n",
    "            self.idx_to_node_id[self.id_counter] = n.id\n",
    "            self.id_counter += 1\n",
    "\n",
    "        def way(self, w):\n",
    "            node_refs = [node.ref for node in w.nodes]\n",
    "\n",
    "            for i in range(len(node_refs) - 1):\n",
    "                node_start = node_refs[i]\n",
    "                node_end = node_refs[i + 1]\n",
    "\n",
    "                node_1_idx = self.node_id_to_idx[node_start]\n",
    "                node_2_idx = self.node_id_to_idx[node_end]\n",
    "\n",
    "                self.edges[0].append(node_1_idx)\n",
    "                self.edges[1].append(node_2_idx)\n",
    "\n",
    "                node_1 = self.nodes[node_1_idx]\n",
    "                node_2 = self.nodes[node_2_idx]\n",
    "\n",
    "                n1_lat, n1_lon = node_1\n",
    "                n2_lat, n2_lon = node_2\n",
    "\n",
    "                dist = compute_distance(n1_lat, n1_lon, n2_lat, n2_lon)\n",
    "                self.edge_dist.append(dist)\n",
    "\n",
    "    mapCreator = MapCreationHandler()\n",
    "    mapCreator.apply_file(osmPath, locations=True)\n",
    "\n",
    "    x = torch.tensor(mapCreator.nodes, dtype=torch.float)\n",
    "    edge_index = torch.tensor(mapCreator.edges, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(mapCreator.edge_dist, dtype=torch.float).unsqueeze(1)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data, mapCreator.node_id_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathDataset(Dataset):\n",
    "\n",
    "    def __init__(self, route_files: List[str], node_id_to_idx: dict, shuffle_waypoints: bool = True, fixed_length: int = 8):\n",
    "        self.route_files = route_files\n",
    "        self.node_id_to_idx = node_id_to_idx\n",
    "        self.shuffle_waypoints = shuffle_waypoints\n",
    "        self.fixed_length = fixed_length\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        samples = []\n",
    "        for f in self.route_files:\n",
    "            with open(f, 'r') as json_file:\n",
    "                route_info = json.load(json_file)\n",
    "\n",
    "            start_id = route_info[\"start\"]\n",
    "            end_id = route_info[\"end\"]\n",
    "            waypoint_tags = route_info[\"waypointTags\"]\n",
    "            waypoint_ids = [tag.split('=')[1] for tag in waypoint_tags]\n",
    "\n",
    "            try:\n",
    "                start_idx = self.node_id_to_idx[int(start_id)]\n",
    "                end_idx = self.node_id_to_idx[int(end_id)]\n",
    "                waypoints_correct = [self.node_id_to_idx[int(w_id)] for w_id in waypoint_ids]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            samples.append({\n",
    "                \"start_idx\": start_idx,\n",
    "                \"waypoints_correct\": waypoints_correct,\n",
    "                \"end_idx\": end_idx\n",
    "            })\n",
    "        return samples\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.data[idx]\n",
    "        start_idx = torch.tensor(sample[\"start_idx\"], dtype=torch.long)\n",
    "        end_idx = torch.tensor(sample[\"end_idx\"], dtype=torch.long)\n",
    "        waypoints_correct = sample[\"waypoints_correct\"]\n",
    "        waypoints_correct_tensor = torch.tensor(waypoints_correct, dtype=torch.long)\n",
    "\n",
    "        # Pad way points if length is different\n",
    "        if len(waypoints_correct) < self.fixed_length:\n",
    "            padding_needed = self.fixed_length - len(waypoints_correct)\n",
    "            waypoints_correct = waypoints_correct + [end_idx.item()] * padding_needed\n",
    "\n",
    "        if self.shuffle_waypoints and len(waypoints_correct) > 1:\n",
    "            waypoints_shuffled = waypoints_correct[:]\n",
    "\n",
    "            # Random change node order\n",
    "            random.shuffle(waypoints_shuffled)\n",
    "            waypoints_shuffled_tensor = torch.tensor(waypoints_shuffled, dtype=torch.long)\n",
    "        else:\n",
    "            waypoints_shuffled_tensor = waypoints_correct_tensor.clone()\n",
    "\n",
    "        return start_idx, waypoints_shuffled_tensor, waypoints_correct_tensor, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path_data(route_dir: str, node_id_to_idx: dict, train_ratio=0.8, val_ratio=0.1, seed=42):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    route_files = [os.path.join(route_dir, f) for f in os.listdir(route_dir) if f.endswith('.json')]\n",
    "    full_dataset = PathDataset(route_files, node_id_to_idx, shuffle_waypoints=False)\n",
    "    dataset_len = len(full_dataset)\n",
    "    train_len = int(dataset_len * train_ratio)\n",
    "    val_len = int(dataset_len * val_ratio)\n",
    "    test_len = dataset_len - train_len - val_len\n",
    "\n",
    "    train_dataset_raw, val_dataset_raw, test_dataset_raw = random_split(full_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "    train_route_files = [route_files[i] for i in train_dataset_raw.indices]\n",
    "    val_route_files = [route_files[i] for i in val_dataset_raw.indices]\n",
    "    test_route_files = [route_files[i] for i in test_dataset_raw.indices]\n",
    "\n",
    "    train_dataset = PathDataset(train_route_files, node_id_to_idx, shuffle_waypoints=True)\n",
    "    val_dataset = PathDataset(val_route_files, node_id_to_idx, shuffle_waypoints=False)\n",
    "    test_dataset = PathDataset(test_route_files, node_id_to_idx, shuffle_waypoints=False)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, beta=True, heads=1):\n",
    "        super(GTN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize transformer convolution layers with edge attributes\n",
    "        conv_layers = [TransformerConv(input_dim, hidden_dim // heads, heads=heads, edge_dim=1, beta=beta)]\n",
    "        conv_layers += [TransformerConv(hidden_dim, hidden_dim // heads, heads=heads, edge_dim=1, beta=beta) for _ in range(num_layers - 2)]\n",
    "        conv_layers.append(TransformerConv(hidden_dim, output_dim, heads=heads, edge_dim=1, beta=beta, concat=True))\n",
    "        self.convs = torch.nn.ModuleList(conv_layers)\n",
    "\n",
    "        # Initialize LayerNorm layers for normalization\n",
    "        norm_layers = [torch.nn.LayerNorm(hidden_dim) for _ in range(num_layers - 1)]\n",
    "        self.norms = torch.nn.ModuleList(norm_layers)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Resets parameters for the convolutional and normalization layers.\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for norm in self.norms:\n",
    "            norm.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        Forward pass with edge attributes.\n",
    "        - x: Node features\n",
    "        - edge_index: Edge indices\n",
    "        - edge_attr: Edge attributes\n",
    "        \"\"\"\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)  # Include edge_attr\n",
    "            x = self.norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Last layer, average multi-head output.\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)  # Include edge_attr\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, max_seq_len=48, ff_dim=2048, dropout=0.1):\n",
    "        super(NodeTransformer, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Learnable special embeddings for fixed start and end node\n",
    "        self.start_node_embed_tag = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.end_node_embed_tag = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=ff_dim, \n",
    "                dropout=dropout, \n",
    "                activation='gelu'\n",
    "            ) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # LayerNorm to stabilize the output\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, waypoint_node_embeds, start_node_embed, end_node_embed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waypoint_node_embeds: Tensor of shape (batch_size, seq_len, embed_dim), where seq_len <= max_seq_len.\n",
    "            start_node_embed: Tensor of shape (batch_size, 1, embed_dim) representing the first fixed node embedding.\n",
    "            end_node_embed: Tensor of shape (batch_size, 1, embed_dim) representing the second fixed node embedding.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len + 2, embed_dim).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = waypoint_node_embeds.shape\n",
    "        \n",
    "        assert seq_len <= self.max_seq_len, f\"Sequence length should be <= {self.max_seq_len}\"\n",
    "        assert embed_dim == self.embed_dim, f\"Embedding dimension mismatch: {embed_dim} != {self.embed_dim}\"\n",
    "\n",
    "        # Add learnable tags to fixed nodes\n",
    "        start_node_embed = start_node_embed + self.start_node_embed_tag  # Shape: (batch_size, 1, embed_dim)\n",
    "        end_node_embed = end_node_embed + self.end_node_embed_tag  # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Concatenate fixed nodes with the variable-length sequence\n",
    "        fixed_nodes = torch.cat([start_node_embed, end_node_embed], dim=1)  # Shape: (batch_size, 2, embed_dim)\n",
    "        full_sequence = torch.cat([fixed_nodes, waypoint_node_embeds], dim=1)  # Shape: (batch_size, seq_len+2, embed_dim)\n",
    "        \n",
    "        # Pass through the Transformer encoder layers\n",
    "        x = full_sequence\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply LayerNorm\n",
    "        x = self.norm(x)\n",
    "        # x = self.head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and graph\n",
    "batch_size = 32\n",
    "graph_path = \"data/stanford.pbf\"\n",
    "route_dir = \"dataprocessing/out\"\n",
    "graph, node_id_to_idx = construct_graph(graph_path)\n",
    "train_dataset, val_dataset, test_dataset = load_path_data(route_dir=route_dir, node_id_to_idx=node_id_to_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(f\"train_dataset length: {len(train_dataset)}\")\n",
    "print(f\"val_dataset length: {len(val_dataset)}\")\n",
    "print(f\"test_dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTN(input_dim=2, hidden_dim=10, output_dim=10, num_layers=2, dropout=0.1, beta=True, heads=1)\n",
    "node_embeddings = model(graph.x, graph.edge_index, graph.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 48\n",
    "embed_dim = 10\n",
    "node_transformer_model = NodeTransformer(embed_dim=embed_dim, num_heads=1, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node_idx = 13\n",
    "end_node_idx = 14\n",
    "waypoint_node_indices = [10, 20, 30]\n",
    "\n",
    "start_node_embed = node_embeddings[start_node_idx].unsqueeze(0).unsqueeze(0)\n",
    "end_node_embed = node_embeddings[end_node_idx].unsqueeze(0).unsqueeze(0)\n",
    "waypoint_node_embeds = node_embeddings[waypoint_node_indices].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = node_transformer_model(waypoint_node_embeds, start_node_embed, end_node_embed)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs224w",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d6facec511a30c804d9144d1187ba6767cc26cf6f517ea6a6972757955b6b89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
