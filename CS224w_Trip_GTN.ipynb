{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "from math import asin, cos, radians, sin, sqrt\n",
    "\n",
    "import osmium\n",
    "from osmium import osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS_EARTH = 6371000\n",
    "\n",
    "def compute_distance(n1_longitude, n1_latitude, n2_longitude, n2_latitude) -> float:\n",
    "    lon1, lat1 = radians(n1_longitude), radians(n1_latitude)\n",
    "    lon2, lat2 = radians(n2_longitude), radians(n2_latitude)\n",
    "\n",
    "    # Haversine formula\n",
    "    deltaLon, deltaLat = lon2 - lon1, lat2 - lat1\n",
    "    haversine = (sin(deltaLat / 2) ** 2) + (cos(lat1) * cos(lat2)) * (\n",
    "        sin(deltaLon / 2) ** 2\n",
    "    )\n",
    "\n",
    "    # Return distance d (factor in radius of earth in meters)\n",
    "    return 2 * RADIUS_EARTH * asin(sqrt(haversine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(osmPath: str):\n",
    "    class MapCreationHandler(osmium.SimpleHandler):\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.nodes = []\n",
    "            self.edges = [[], []]\n",
    "            self.edge_dist = []\n",
    "\n",
    "            self.node_id_to_idx = {}\n",
    "            self.idx_to_node_id = {}\n",
    "            self.id_counter = 0\n",
    "\n",
    "        def node(self, n: osm.Node) -> None:\n",
    "            self.nodes.append([n.location.lat, n.location.lon])\n",
    "            self.node_id_to_idx[n.id] = self.id_counter\n",
    "            self.idx_to_node_id[self.id_counter] = n.id\n",
    "            self.id_counter += 1\n",
    "\n",
    "        def way(self, w):\n",
    "            node_refs = [node.ref for node in w.nodes]\n",
    "\n",
    "            for i in range(len(node_refs) - 1):\n",
    "                node_start = node_refs[i]\n",
    "                node_end = node_refs[i + 1]\n",
    "                \n",
    "                node_1_idx = self.node_id_to_idx[node_start]\n",
    "                node_2_idx = self.node_id_to_idx[node_end]\n",
    "\n",
    "                self.edges[0].append(node_1_idx)\n",
    "                self.edges[1].append(node_2_idx)\n",
    "\n",
    "                node_1 = self.nodes[node_1_idx]\n",
    "                node_2 = self.nodes[node_2_idx]\n",
    "\n",
    "                n1_longitude, n1_latitude = node_1\n",
    "                n2_longitude, n2_latitude = node_2\n",
    "\n",
    "                dist = compute_distance(n1_longitude, n1_latitude, n2_longitude, n2_latitude)\n",
    "                self.edge_dist.append(dist)\n",
    "\n",
    "    mapCreator = MapCreationHandler()\n",
    "    mapCreator.apply_file(osmPath, locations=True)\n",
    "\n",
    "    x = torch.tensor(mapCreator.nodes, dtype=torch.float)\n",
    "    edge_index = torch.tensor(mapCreator.edges, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(mapCreator.edge_dist, dtype=torch.float).unsqueeze(1)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr,)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  37.4340, -122.1725],\n",
      "        [  37.4342, -122.1726],\n",
      "        [  37.4344, -122.1724],\n",
      "        ...,\n",
      "        [  37.4312, -122.1713],\n",
      "        [  37.4312, -122.1712],\n",
      "        [  37.4314, -122.1710]])\n",
      "tensor([[13919, 13766, 13981,  ..., 23687, 23688, 23689],\n",
      "        [13766, 13981, 20493,  ..., 23006, 23689, 19361]])\n",
      "tensor([[ 8.3168],\n",
      "        [ 7.1985],\n",
      "        [52.1610],\n",
      "        ...,\n",
      "        [ 2.1193],\n",
      "        [12.8045],\n",
      "        [10.2401]])\n"
     ]
    }
   ],
   "source": [
    "graph = construct_graph(\"data/stanford.pbf\")\n",
    "print(graph.x)\n",
    "print(graph.edge_index)\n",
    "print(graph.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, beta=True, heads=1):\n",
    "        super(GTN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize transformer convolution layers with edge attributes\n",
    "        conv_layers = [TransformerConv(input_dim, hidden_dim // heads, heads=heads, edge_dim=1, beta=beta)]\n",
    "        conv_layers += [TransformerConv(hidden_dim, hidden_dim // heads, heads=heads, edge_dim=1, beta=beta) for _ in range(num_layers - 2)]\n",
    "        conv_layers.append(TransformerConv(hidden_dim, output_dim, heads=heads, edge_dim=1, beta=beta, concat=True))\n",
    "        self.convs = torch.nn.ModuleList(conv_layers)\n",
    "\n",
    "        # Initialize LayerNorm layers for normalization\n",
    "        norm_layers = [torch.nn.LayerNorm(hidden_dim) for _ in range(num_layers - 1)]\n",
    "        self.norms = torch.nn.ModuleList(norm_layers)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Resets parameters for the convolutional and normalization layers.\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for norm in self.norms:\n",
    "            norm.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        Forward pass with edge attributes.\n",
    "        - x: Node features\n",
    "        - edge_index: Edge indices\n",
    "        - edge_attr: Edge attributes\n",
    "        \"\"\"\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)  # Include edge_attr\n",
    "            x = self.norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Last layer, average multi-head output.\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)  # Include edge_attr\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTN(input_dim=2, hidden_dim=10, output_dim=10, num_layers=2,\n",
    "            dropout=0.1, beta=True, heads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23691, 10])\n"
     ]
    }
   ],
   "source": [
    "node_embeddings = model(graph.x, graph.edge_index, graph.edge_attr)\n",
    "print(node_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, max_seq_len=48, ff_dim=2048, dropout=0.1):\n",
    "        super(NodeTransformer, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Learnable special embeddings for fixed start and end node\n",
    "        self.start_node_embed_tag = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.end_node_embed_tag = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=ff_dim, \n",
    "                dropout=dropout, \n",
    "                activation='gelu'\n",
    "            ) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # LayerNorm to stabilize the output\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, waypoint_node_embeds, start_node_embed, end_node_embed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waypoint_node_embeds: Tensor of shape (batch_size, seq_len, embed_dim), where seq_len <= max_seq_len.\n",
    "            start_node_embed: Tensor of shape (batch_size, 1, embed_dim) representing the first fixed node embedding.\n",
    "            end_node_embed: Tensor of shape (batch_size, 1, embed_dim) representing the second fixed node embedding.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len + 2, embed_dim).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = waypoint_node_embeds.shape\n",
    "        \n",
    "        assert seq_len <= self.max_seq_len, f\"Sequence length should be <= {self.max_seq_len}\"\n",
    "        assert embed_dim == self.embed_dim, f\"Embedding dimension mismatch: {embed_dim} != {self.embed_dim}\"\n",
    "\n",
    "        # Add learnable tags to fixed nodes\n",
    "        start_node_embed = start_node_embed + self.start_node_embed_tag  # Shape: (batch_size, 1, embed_dim)\n",
    "        end_node_embed = end_node_embed + self.end_node_embed_tag  # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Concatenate fixed nodes with the variable-length sequence\n",
    "        fixed_nodes = torch.cat([start_node_embed, end_node_embed], dim=1)  # Shape: (batch_size, 2, embed_dim)\n",
    "        full_sequence = torch.cat([fixed_nodes, waypoint_node_embeds], dim=1)  # Shape: (batch_size, seq_len+2, embed_dim)\n",
    "        \n",
    "        # Pass through the Transformer encoder layers\n",
    "        x = full_sequence\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply LayerNorm\n",
    "        x = self.norm(x)\n",
    "        # x = self.head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 48\n",
    "embed_dim = 10\n",
    "node_transformer_model = NodeTransformer(embed_dim=embed_dim, num_heads=1, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node_idx = 13\n",
    "end_node_idx = 14\n",
    "waypoint_node_indices = [10, 20, 30]\n",
    "\n",
    "start_node_embed = node_embeddings[start_node_idx].unsqueeze(0).unsqueeze(0)\n",
    "end_node_embed = node_embeddings[end_node_idx].unsqueeze(0).unsqueeze(0)\n",
    "waypoint_node_embeds = node_embeddings[waypoint_node_indices].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = node_transformer_model(waypoint_node_embeds, start_node_embed, end_node_embed)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 9.8694\n",
      "Epoch [2/20], Loss: 8.8360\n",
      "Epoch [3/20], Loss: 8.5482\n",
      "Epoch [4/20], Loss: 8.5193\n",
      "Epoch [5/20], Loss: 8.2930\n",
      "Epoch [6/20], Loss: 8.2387\n",
      "Epoch [7/20], Loss: 8.3118\n",
      "Epoch [8/20], Loss: 8.2731\n",
      "Epoch [9/20], Loss: 8.2657\n",
      "Epoch [10/20], Loss: 8.2668\n",
      "Epoch [11/20], Loss: 8.2569\n",
      "Epoch [12/20], Loss: 8.2494\n",
      "Epoch [13/20], Loss: 8.2760\n",
      "Epoch [14/20], Loss: 8.2552\n",
      "Epoch [15/20], Loss: 8.2524\n",
      "Epoch [16/20], Loss: 8.2500\n",
      "Epoch [17/20], Loss: 8.2500\n",
      "Epoch [18/20], Loss: 8.2500\n",
      "Epoch [19/20], Loss: 8.2500\n",
      "Epoch [20/20], Loss: 8.2500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Training parameters\n",
    "input_dim = 2      \n",
    "hidden_dim = 64\n",
    "output_dim = 32\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "batch_size = 1 \n",
    "\n",
    "device = \"cpu\"\n",
    "model = GTN(input_dim, hidden_dim, output_dim, num_layers, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def loss_(scores, target_order):\n",
    "   \n",
    "   # TODO: How should we handle the loss using score and target?\n",
    "    target_ranks = torch.arange(len(target_order), dtype=torch.float, device=scores.device)\n",
    "    ordered_scores = scores[target_order]\n",
    "    loss = F.mse_loss(ordered_scores, target_ranks)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)  \n",
    "\n",
    "            scores = model(batch.x, batch.edge_index)\n",
    "            loss = loss_(scores, batch.target_order)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "train(model, data_loader, optimizer, num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
