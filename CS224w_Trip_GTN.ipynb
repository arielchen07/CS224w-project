{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import osmium\n",
    "from osmium import osm\n",
    "from typing import List, Tuple\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "\n",
    "R = 6371000  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(osmPath: str):\n",
    "\n",
    "    def compute_distance(lat1, lon1, lat2, lon2):\n",
    "        phi1 = math.radians(lat1)\n",
    "        phi2 = math.radians(lat2)\n",
    "        dphi = math.radians(lat2 - lat1)\n",
    "        dlambda = math.radians(lon2 - lon1)\n",
    "        a = (math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2)\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        return R * c\n",
    "\n",
    "    class MapCreationHandler(osmium.SimpleHandler):\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.nodes = []\n",
    "            self.edges = [[], []]\n",
    "            self.edge_dist = []\n",
    "            self.node_id_to_idx = {}\n",
    "            self.idx_to_node_id = {}\n",
    "            self.id_counter = 0\n",
    "\n",
    "        def node(self, n: osmium.osm.Node) -> None:\n",
    "            self.nodes.append([n.location.lat, n.location.lon])\n",
    "            self.node_id_to_idx[n.id] = self.id_counter\n",
    "            self.idx_to_node_id[self.id_counter] = n.id\n",
    "            self.id_counter += 1\n",
    "\n",
    "        def way(self, w):\n",
    "            node_refs = [node.ref for node in w.nodes]\n",
    "\n",
    "            for i in range(len(node_refs) - 1):\n",
    "                node_start = node_refs[i]\n",
    "                node_end = node_refs[i + 1]\n",
    "\n",
    "                node_1_idx = self.node_id_to_idx[node_start]\n",
    "                node_2_idx = self.node_id_to_idx[node_end]\n",
    "\n",
    "                self.edges[0].append(node_1_idx)\n",
    "                self.edges[1].append(node_2_idx)\n",
    "\n",
    "                node_1 = self.nodes[node_1_idx]\n",
    "                node_2 = self.nodes[node_2_idx]\n",
    "\n",
    "                n1_lat, n1_lon = node_1\n",
    "                n2_lat, n2_lon = node_2\n",
    "\n",
    "                dist = compute_distance(n1_lat, n1_lon, n2_lat, n2_lon)\n",
    "                self.edge_dist.append(dist)\n",
    "\n",
    "    mapCreator = MapCreationHandler()\n",
    "    mapCreator.apply_file(osmPath, locations=True)\n",
    "\n",
    "    x = torch.tensor(mapCreator.nodes, dtype=torch.float)\n",
    "    edge_index = torch.tensor(mapCreator.edges, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(mapCreator.edge_dist, dtype=torch.float).unsqueeze(1)\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data, mapCreator.node_id_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathDataset(Dataset):\n",
    "\n",
    "    def __init__(self, route_files: List[str], node_id_to_idx: dict, shuffle_waypoints: bool = True, fixed_length: int = 8):\n",
    "        self.route_files = route_files\n",
    "        self.node_id_to_idx = node_id_to_idx\n",
    "        self.shuffle_waypoints = shuffle_waypoints\n",
    "        self.fixed_length = fixed_length\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        samples = []\n",
    "        for f in self.route_files:\n",
    "            with open(f, 'r') as json_file:\n",
    "                route_info = json.load(json_file)\n",
    "\n",
    "            start_id = route_info[\"start\"]\n",
    "            end_id = route_info[\"end\"]\n",
    "            waypoint_tags = route_info[\"waypointTags\"]\n",
    "            waypoint_ids = [tag.split('=')[1] for tag in waypoint_tags]\n",
    "\n",
    "            try:\n",
    "                start_idx = self.node_id_to_idx[int(start_id)]\n",
    "                end_idx = self.node_id_to_idx[int(end_id)]\n",
    "                waypoints_correct = [self.node_id_to_idx[int(w_id)] for w_id in waypoint_ids]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            samples.append({\n",
    "                \"start_idx\": start_idx,\n",
    "                \"waypoints_correct\": waypoints_correct,\n",
    "                \"end_idx\": end_idx\n",
    "            })\n",
    "        return samples\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.data[idx]\n",
    "        start_idx = torch.tensor(sample[\"start_idx\"], dtype=torch.long)\n",
    "        end_idx = torch.tensor(sample[\"end_idx\"], dtype=torch.long)\n",
    "        waypoints_correct = sample[\"waypoints_correct\"]\n",
    "\n",
    "        # Pad way points if length is different\n",
    "        if len(waypoints_correct) < self.fixed_length:\n",
    "            padding_needed = self.fixed_length - len(waypoints_correct)\n",
    "            waypoints_correct = waypoints_correct + [end_idx.item()] * padding_needed\n",
    "\n",
    "        waypoints_correct_tensor = torch.tensor(waypoints_correct, dtype=torch.long)\n",
    "\n",
    "        if self.shuffle_waypoints and len(waypoints_correct) > 1:\n",
    "            waypoints_shuffled = waypoints_correct[:]\n",
    "\n",
    "            # Random change node order\n",
    "            random.shuffle(waypoints_shuffled)\n",
    "            waypoints_shuffled_tensor = torch.tensor(waypoints_shuffled, dtype=torch.long)\n",
    "        else:\n",
    "            waypoints_shuffled_tensor = waypoints_correct_tensor.clone()\n",
    "\n",
    "        return start_idx, waypoints_shuffled_tensor, waypoints_correct_tensor, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path_data(route_dir: str, node_id_to_idx: dict, train_ratio=0.8, val_ratio=0.1, seed=42):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    route_files = [os.path.join(route_dir, f) for f in os.listdir(route_dir) if f.endswith('.json')]\n",
    "    full_dataset = PathDataset(route_files, node_id_to_idx, shuffle_waypoints=False)\n",
    "    dataset_len = len(full_dataset)\n",
    "    train_len = int(dataset_len * train_ratio)\n",
    "    val_len = int(dataset_len * val_ratio)\n",
    "    test_len = dataset_len - train_len - val_len\n",
    "\n",
    "    train_dataset_raw, val_dataset_raw, test_dataset_raw = random_split(full_dataset, [train_len, val_len, test_len])\n",
    "\n",
    "    train_route_files = [route_files[i] for i in train_dataset_raw.indices]\n",
    "    val_route_files = [route_files[i] for i in val_dataset_raw.indices]\n",
    "    test_route_files = [route_files[i] for i in test_dataset_raw.indices]\n",
    "\n",
    "    train_dataset = PathDataset(train_route_files, node_id_to_idx, shuffle_waypoints=True)\n",
    "    val_dataset = PathDataset(val_route_files, node_id_to_idx, shuffle_waypoints=False)\n",
    "    test_dataset = PathDataset(test_route_files, node_id_to_idx, shuffle_waypoints=False)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, beta=True, heads=1):\n",
    "        super(GTN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize transformer convolution layers with edge attributes\n",
    "        conv_layers = [TransformerConv(input_dim, hidden_dim // heads, heads=heads, edge_dim=1, beta=beta)]\n",
    "        conv_layers += [TransformerConv(hidden_dim, hidden_dim // heads, heads=heads, edge_dim=1, beta=beta) for _ in range(num_layers - 2)]\n",
    "        conv_layers.append(TransformerConv(hidden_dim, output_dim, heads=heads, edge_dim=1, beta=beta, concat=True))\n",
    "        self.convs = torch.nn.ModuleList(conv_layers)\n",
    "\n",
    "        # Initialize LayerNorm layers for normalization\n",
    "        norm_layers = [torch.nn.LayerNorm(hidden_dim) for _ in range(num_layers - 1)]\n",
    "        self.norms = torch.nn.ModuleList(norm_layers)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Resets parameters for the convolutional and normalization layers.\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for norm in self.norms:\n",
    "            norm.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        Forward pass with edge attributes.\n",
    "        - x: Node features\n",
    "        - edge_index: Edge indices\n",
    "        - edge_attr: Edge attributes\n",
    "        \"\"\"\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)  # Include edge_attr\n",
    "            x = self.norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Last layer, average multi-head output.\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)  # Include edge_attr\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, max_seq_len=48, ff_dim=2048, dropout=0.1):\n",
    "        super(NodeTransformer, self).__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Learnable special embeddings for fixed start and end node\n",
    "        self.start_node_embed_tag = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.end_node_embed_tag = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=ff_dim, \n",
    "                dropout=dropout, \n",
    "                activation='gelu'\n",
    "            ) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # LayerNorm to stabilize the output\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, waypoint_node_embeds, start_node_embed, end_node_embed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waypoint_node_embeds: Tensor of shape (batch_size, seq_len, embed_dim), where seq_len <= max_seq_len.\n",
    "            start_node_embed: Tensor of shape (batch_size, 1, embed_dim) representing the first fixed node embedding.\n",
    "            end_node_embed: Tensor of shape (batch_size, 1, embed_dim) representing the second fixed node embedding.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len + 2, embed_dim).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = waypoint_node_embeds.shape\n",
    "        \n",
    "        assert seq_len <= self.max_seq_len, f\"Sequence length should be <= {self.max_seq_len}\"\n",
    "        assert embed_dim == self.embed_dim, f\"Embedding dimension mismatch: {embed_dim} != {self.embed_dim}\"\n",
    "\n",
    "        # Add learnable tags to fixed nodes\n",
    "        start_node_embed = start_node_embed + self.start_node_embed_tag  # Shape: (batch_size, 1, embed_dim)\n",
    "        end_node_embed = end_node_embed + self.end_node_embed_tag  # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Concatenate fixed nodes with the variable-length sequence\n",
    "        fixed_nodes = torch.cat([start_node_embed, end_node_embed], dim=1)  # Shape: (batch_size, 2, embed_dim)\n",
    "        full_sequence = torch.cat([fixed_nodes, waypoint_node_embeds], dim=1)  # Shape: (batch_size, seq_len+2, embed_dim)\n",
    "        \n",
    "        # Pass through the Transformer encoder layers\n",
    "        x = full_sequence\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply LayerNorm\n",
    "        x = self.norm(x)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset length: 785\n",
      "val_dataset length: 98\n",
      "test_dataset length: 99\n"
     ]
    }
   ],
   "source": [
    "# Load data and graph\n",
    "batch_size = 1\n",
    "graph_path = \"data/stanford.pbf\"\n",
    "route_dir = \"dataprocessing/out\"\n",
    "graph, node_id_to_idx = construct_graph(graph_path)\n",
    "train_dataset, val_dataset, test_dataset = load_path_data(route_dir=route_dir, node_id_to_idx=node_id_to_idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(f\"train_dataset length: {len(train_dataset)}\")\n",
    "print(f\"val_dataset length: {len(val_dataset)}\")\n",
    "print(f\"test_dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTTP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GTTP, self).__init__()\n",
    "\n",
    "        self.gtn = GTN(input_dim=2, hidden_dim=10, output_dim=10, num_layers=2, dropout=0.1, beta=True, heads=1)\n",
    "        self.node_transformer_model = NodeTransformer(embed_dim=embed_dim, num_heads=1, num_layers=4)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, start_idx, end_idx, waypoint_node_indices):\n",
    "        node_embeddings = self.gtn(x, edge_index, edge_attr)\n",
    "        # node_embeddings = self.gtn()\n",
    "        start_node_embed = node_embeddings[start_idx].unsqueeze(1)\n",
    "        end_node_embed = node_embeddings[end_idx].unsqueeze(1)\n",
    "        waypoint_node_embeds = node_embeddings[waypoint_node_indices]\n",
    "        pred = self.node_transformer_model(waypoint_node_embeds, start_node_embed, end_node_embed)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTTP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stabilized Kendall Tau Loss: 0.5412\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StabilizedKendallTauLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StabilizedKendallTauLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Stabilized Kendall Tau Loss (differentiable approximation)\n",
    "        \"\"\"\n",
    "        y_pred = y_pred.float()\n",
    "        y_true = y_true.float()\n",
    "        \n",
    "        # Normalize predictions\n",
    "        y_pred = (y_pred - y_pred.mean(dim=1, keepdim=True)) / (y_pred.std(dim=1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Pairwise differences\n",
    "        diff_pred = y_pred.unsqueeze(2) - y_pred.unsqueeze(1)\n",
    "        diff_true = y_true.unsqueeze(2) - y_true.unsqueeze(1)\n",
    "        \n",
    "        # Clamping to prevent overflow\n",
    "        diff_pred = torch.clamp(diff_pred, -15, 15)\n",
    "        \n",
    "        # Pairwise product\n",
    "        pairwise_product = diff_true * diff_pred\n",
    "        \n",
    "        # Stabilized sigmoid loss\n",
    "        loss = torch.log1p(torch.exp(-pairwise_product))\n",
    "        \n",
    "        # Masking diagonal\n",
    "        batch_size, num_items, _ = loss.shape\n",
    "        mask = ~torch.eye(num_items, dtype=torch.bool, device=loss.device)\n",
    "        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        loss = loss[mask].view(batch_size, -1)\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "y_pred = torch.tensor([[100000.0, -100000.0, 500.0], [-200000.0, 4000.0, 3000.0]])  # Extreme predictions\n",
    "y_true = torch.tensor([[3.0, 1.0, 2.0], [2.0, 4.0, 1.0]])  # True scores\n",
    "\n",
    "loss_fn = StabilizedKendallTauLoss()\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "print(f\"Stabilized Kendall Tau Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stabilized Kendall Tau Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "y_pred = torch.tensor([[-0.4972, 1, 1]])\n",
    "y_true = torch.tensor([[ 4465.,  1535., 15142.]])\n",
    "\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "print(f\"Stabilized Kendall Tau Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class KendallTauLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KendallTauLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Kendall Tau Loss (differentiable approximation)\n",
    "        \n",
    "        Parameters:\n",
    "        - y_pred: Predicted scores (torch.Tensor of shape [batch_size, num_items])\n",
    "        - y_true: True scores or rankings (torch.Tensor of shape [batch_size, num_items])\n",
    "        \n",
    "        Returns:\n",
    "        - loss: Computed Kendall Tau loss (scalar)\n",
    "        \"\"\"\n",
    "        # Ensure the inputs are float tensors\n",
    "        y_pred = y_pred.float()\n",
    "        y_true = y_true.float()\n",
    "        \n",
    "        # Create pairwise differences for predictions and ground truth\n",
    "        diff_pred = y_pred.unsqueeze(2) - y_pred.unsqueeze(1)  # Shape: [batch_size, num_items, num_items]\n",
    "        diff_true = y_true.unsqueeze(2) - y_true.unsqueeze(1)  # Shape: [batch_size, num_items, num_items]\n",
    "        \n",
    "        # Compute pairwise agreements: (y_i - y_j) * (s_i - s_j)\n",
    "        pairwise_product = diff_true * diff_pred  # Shape: [batch_size, num_items, num_items]\n",
    "        \n",
    "        # Compute pairwise loss using logistic sigmoid approximation\n",
    "        loss = torch.log(1 + torch.exp(-pairwise_product))\n",
    "        \n",
    "        # Exclude diagonal elements (self-comparisons)\n",
    "        batch_size, num_items, _ = loss.shape\n",
    "        mask = ~torch.eye(num_items, dtype=torch.bool, device=loss.device)  # Shape: [num_items, num_items]\n",
    "        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        loss = loss[mask].view(batch_size, -1)\n",
    "        \n",
    "        # Return the mean loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4972, -0.9163, -0.4660, -0.7453, -0.7843, -0.9008, -0.7679, -1.4530]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "tensor([[ 4465.,  1535., 15142., 16012., 17906., 18829., 19166., 21121.]])\n",
      "Epoch [1/1], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Move model to appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a proper loss function\n",
    "loss_fn = StabilizedKendallTauLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        start_idx, waypoints_shuffled, waypoints_correct, end_idx = [x.to(device) for x in batch]\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_ordering = model(\n",
    "            graph.x, graph.edge_index, graph.edge_attr, start_idx, end_idx, waypoints_shuffled\n",
    "        )\n",
    "\n",
    "        # Process the predictions\n",
    "        predicted_ordering = predicted_ordering[:, 2:].squeeze(2)\n",
    "\n",
    "        print(predicted_ordering)\n",
    "        print(waypoints_correct.float())\n",
    "        break\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(predicted_ordering, waypoints_correct.float())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for tracking\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Accumulate loss for tracking\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(node_transformer_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Move model to appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "node_transformer_model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    node_transformer_model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        start_idx, waypoints_shuffled, waypoints_correct, end_idx = [x.to(device) for x in batch]\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_ordering = node_transformer_model(\n",
    "            node_embeddings[waypoints_shuffled], \n",
    "            node_embeddings[start_idx].unsqueeze(1), \n",
    "            node_embeddings[end_idx].unsqueeze(1)\n",
    "        )\n",
    "\n",
    "        # Process the predictions\n",
    "        predicted_ordering = predicted_ordering[:, 2:].squeeze(2)\n",
    "\n",
    "        # Compute loss\n",
    "        # loss = kendall_tau_loss(predicted_ordering, waypoints_correct)\n",
    "\n",
    "        loss = loss_fn(predicted_ordering, waypoints_correct.float())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for tracking\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embeddings[waypoints_shuffled].shape\n",
    "node_embeddings[start_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node_idx = 13\n",
    "end_node_idx = 14\n",
    "waypoint_node_indices = [10, 20, 30]\n",
    "\n",
    "start_node_embed = node_embeddings[start_node_idx].unsqueeze(0).unsqueeze(0)\n",
    "end_node_embed = node_embeddings[end_node_idx].unsqueeze(0).unsqueeze(0)\n",
    "waypoint_node_embeds = node_embeddings[waypoint_node_indices].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = node_transformer_model(waypoint_node_embeds, start_node_embed, end_node_embed)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
